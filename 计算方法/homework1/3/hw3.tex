\documentclass[a4paper]{book}

\usepackage{geometry}
% make full use of A4 papers
\geometry{margin=1.5cm, vmargin={0pt,1cm}}
\setlength{\topmargin}{-1cm}
\setlength{\paperheight}{29.7cm}
\setlength{\textheight}{25.1cm}

% auto adjust the marginals
\usepackage{marginfix}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{CJKutf8}   % for Chinese characters
\usepackage{ctex}
\usepackage{enumerate}
\usepackage{graphicx}  % for figures
\usepackage{layout}
\usepackage{multicol}  % multiple columns to reduce number of pages
\usepackage{mathrsfs}  
\usepackage{fancyhdr}
\usepackage{subfigure}
\usepackage{tcolorbox}
\usepackage{tikz-cd}
\usepackage{listings}
\usepackage{xcolor} %代码高亮
\usepackage{braket}
\usepackage{algorithm} 
\usepackage{algorithmicx}  
\usepackage{algpseudocode}  
\usepackage{amsmath}  

\floatname{algorithm}{算法}  
\renewcommand{\algorithmicrequire}{\textbf{输入:}}  
\renewcommand{\algorithmicensure}{\textbf{输出:}}  
\renewcommand{\algorithmicrequire}{\textbf{Input : }}
\renewcommand{\algorithmicrequire}{\textbf{Precondition : }}
\renewcommand{\algorithmicensure}{\textbf{Output : }}
\renewcommand{\algorithmicensure}{\textbf{Postcondition : }}
%------------------
% common commands %
%------------------
% differentiation
\newcommand{\gen}[1]{\left\langle #1 \right\rangle}
\newcommand{\dif}{\mathrm{d}}
\newcommand{\difPx}[1]{\frac{\partial #1}{\partial x}}
\newcommand{\difPy}[1]{\frac{\partial #1}{\partial y}}
\newcommand{\Dim}{\mathrm{D}}
\newcommand{\avg}[1]{\left\langle #1 \right\rangle}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\Span}{\mathrm{span}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\Arity}{\mathrm{arity}}
\newcommand{\Int}{\mathrm{Int}}
\newcommand{\Ext}{\mathrm{Ext}}
\newcommand{\Cl}{\mathrm{Cl}}
\newcommand{\Fr}{\mathrm{Fr}}
% group is generated by
\newcommand{\grb}[1]{\left\langle #1 \right\rangle}
% rank
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\Iden}{\mathrm{Id}}

% this environment is for solutions of examples and exercises
\newenvironment{solution}%
{\noindent\textbf{Solution.}}%
{\qedhere}
% Define a Solution environment like proof
\makeatletter
\newenvironment{sol}[1][\solname]{\par
  \pushQED{\qed}
  \normalfont \topsep6\p@\@plus6\p@\relax
  \trivlist
  \item[\hskip\labelsep
        \itshape
    #1\@addpunct{.}]\ignorespaces
}{\popQED\endtrivlist\@endpefalse}
\providecommand{\solname}{Solution}
\makeatother
% the following command is for disabling environments
%  so that their contents do not show up in the pdf.
\makeatletter
\newcommand{\voidenvironment}[1]{%
\expandafter\providecommand\csname env@#1@save@env\endcsname{}%
\expandafter\providecommand\csname env@#1@process\endcsname{}%
\@ifundefined{#1}{}{\RenewEnviron{#1}{}}%
}
\makeatother

%---------------------------------------------
% commands specifically for complex analysis %
%---------------------------------------------
% complex conjugate
\newcommand{\ccg}[1]{\overline{#1}}
% the imaginary unit
\newcommand{\ii}{\mathbf{i}}
%\newcommand{\ii}{\boldsymbol{i}}
% the real part
\newcommand{\Rez}{\mathrm{Re}\,}
% the imaginary part
\newcommand{\Imz}{\mathrm{Im}\,}
% punctured complex plane
\newcommand{\pcp}{\mathbb{C}^{\bullet}}
% the principle branch of the logarithm
\newcommand{\Log}{\mathrm{Log}}
% the principle value of a nonzero complex number
\newcommand{\Arg}{\mathrm{Arg}}
\newcommand{\Null}{\mathrm{null}}
\newcommand{\Range}{\mathrm{range}}
\newcommand{\Ker}{\mathrm{ker}}
\newcommand{\Iso}{\mathrm{Iso}}
\newcommand{\Aut}{\mathrm{Aut}}
\newcommand{\ord}{\mathrm{ord}}
\newcommand{\Res}{\mathrm{Res}}
%\newcommand{\GL2R}{\mathrm{GL}(2,\mathbb{R})}
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\SL}{\mathrm{SL}}
\newcommand{\Dist}[2]{\left|{#1}-{#2}\right|}

\newcommand\tbbint{{-\mkern -16mu\int}}
\newcommand\tbint{{\mathchar '26\mkern -14mu\int}}
\newcommand\dbbint{{-\mkern -19mu\int}}
\newcommand\dbint{{\mathchar '26\mkern -18mu\int}}
\newcommand\bint{
{\mathchoice{\dbint}{\tbint}{\tbint}{\tbint}}
}
\newcommand\bbint{
{\mathchoice{\dbbint}{\tbbint}{\tbbint}{\tbbint}}
}





%----------------------------------------
% theorem and theorem-like environments %
%----------------------------------------
\numberwithin{equation}{chapter}
\theoremstyle{definition}

\newtheorem{thm}{Theorem}[chapter]
\newtheorem{axm}[thm]{Axiom}
\newtheorem{alg}[thm]{Algorithm}
\newtheorem{asm}[thm]{Assumption}
\newtheorem{defn}[thm]{Definition}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{rul}[thm]{Rule}
\newtheorem{coro}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{exm}{Example}[chapter]
\newtheorem{rem}{Remark}[chapter]
\newtheorem{exc}[exm]{Exercise}
\newtheorem{frm}[thm]{Formula}
\newtheorem{ntn}{Notation}
\newtheorem{pro}{Problem}

% for complying with the convention in the textbook
\newtheorem{rmk}[thm]{Remark}


%----------------------
% the end of preamble %
%----------------------

\begin{document}

%\tableofcontents
%\clearpage

\pagestyle{fancy}
%\lhead{Qinghai Zhang}
%\chead{Notes on Algebraic Topology}
%\rhead{Fall 2018}


\setcounter{chapter}{2}
\pagenumbering{arabic}
% \setcounter{page}{0}


% each chapter is factored into a separate file.

\chapter{hw3 12235005 谭焱}
\begin{pro}
    Consider the nonlinear equation
    \begin{displaymath}
      f(x) = x^2 - 2 = 0.
    \end{displaymath}
    \begin{itemize}
    \item[(a)]
      With $x_0=1$ as a starting point,
      what is the value of $x_1$ if you use Newton's method for solving this problem?
  
    \item[(b)]
      With $x_0=1$ and $x_1=2$ as starting points,
      what is the value of $x_2$ if you use the secant method for the same problem?
    \end{itemize}
  \end{pro}
  
  \begin{sol}
    \begin{itemize}
    \item[(a)]
      \begin{displaymath}
        x_1 = x_0 - \frac{f(x_0)}{f'(x_0)} = 1 - \frac{1-2}{2} = 1.5.
      \end{displaymath}
  
    \item[(b)]
      \begin{displaymath}
        x_2 = x_1 - f(x_1)\frac{x_1-x_0}{f(x_1)-f(x_0)}
      = 2 - 2 \frac{2-1}{2-(-1)} = \frac{4}{3}.
      \end{displaymath}
    \end{itemize}
  \end{sol}

  \begin{pro}
    Newton's method is sometimes used to implement the built-in
    root function on a computer, with the initial guess
    supplied by a lookup table.
    \begin{itemize}
        \item [(a)] What is the Newton iteration for computing the square root of a
        positive number $y$ (i.e., for solving the equation
        $f(x)=x^2-y=0$, given $y$)?
    \end{itemize}
  \end{pro}
  
  \begin{sol}
    \begin{displaymath}
      x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} = x_n - \frac{x_n^2 - y}{2x_n}
      = \frac{x_n^2 + y}{2x_n}.
    \end{displaymath}
  \end{sol}


  \begin{pro}
    Express the Newton iteration for solving each of the following
    systems of nonlinear equations.
    \begin{itemize}
        \item [(b)]\begin{align*}
          x_1^2 + x_1x_2^3 &= 9, \\
          3x_1^2x_2 - x_2^3 &= 4.
        \end{align*}
    \end{itemize}
  \end{pro}
  
  \begin{sol}
    The Jacobian matrix is 
    \begin{displaymath}
      J(x_1, x_2) =
      \begin{bmatrix}
        2x_1 + x_2^3 & 3x_1x_2^2 \\
        6x_1x_2 & 3x_1^2 - 3x_2^2.
      \end{bmatrix}
    \end{displaymath}
    We have
      \begin{displaymath}
        J(x_1^{(n)}, x_2^{(n)})
        \begin{bmatrix}
          s_1^{(n)} \\
          s_2^{(n)}
        \end{bmatrix}
        = \mathbf{f}(x_1^{(n)}, x_2^{(n)}).
      \end{displaymath}
      So
      \begin{displaymath}
        \begin{bmatrix}
          x_1^{(n+1)} \\
          x_2^{(n+1)}
        \end{bmatrix}
        =
        \begin{bmatrix}
          x_1^{(n)} \\
          x_2^{(n)}
        \end{bmatrix}
        +
        \begin{bmatrix}
          s_1^{(n)} \\
          s_2^{(n)}
        \end{bmatrix}.
        =
        \begin{bmatrix}
            x_1^{(n)} \\
            x_2^{(n)}
          \end{bmatrix}
          +
          \begin{bmatrix}
            2x_1 + x_2^3 & 3x_1x_2^2 \\
            6x_1x_2 & 3x_1^2 - 3x_2^2.
          \end{bmatrix}^{-1}
          \begin{bmatrix}
            x_1^{(n)} \\
            x_2^{(n)}
          \end{bmatrix}
      \end{displaymath}
  \end{sol}

  \begin{pro}
    Suppose you are using the secant method to find a root $x^{*}$ of a
    nonlinear equation $f(x)=0$.
    Show that if at any iteration it happens to be the case that either
    $x_k=x^{*}$ or $x_{k-1}=x^{*}$
    (but not both),
    then it will also be true that $x_{k+1}=x^{*}$.
  \end{pro}
  \begin{sol}
    \begin{itemize}
    \item
      When $x_k=x^{*}$, knowing
      \begin{displaymath}
        x_{k+1} = x_k - f(x_k)\frac{x_k-x_{k-1}}{f(x_k)-f(x_{k-1})}
        = x^{*} - f(x^{*})\frac{x^{*}-x_{k-1}}{f(x^{*})-f(x_{k-1})}
        = x^{*} - 0 = x^{*}.
      \end{displaymath}
  
    \item
      When $x_{k-1}=x^{*}$, knowing
      \begin{align*}
        x_{k+1} &= x_k - f(x_k)\frac{x_k-x_{k-1}}{f(x_k)-f(x_{k-1})}
                  = x_k - f(x_k)\frac{x_k - x^{*}}{f(x_k)-f(x^{*})} \\
        &= x_k - f(x_k) \frac{x_k-x^{*}}{f(x_k)} = x_k - (x_k - x^{*})
        \\
        &= x^{*}.
      \end{align*}
    \end{itemize}
  \end{sol}

  \begin{pro}
    Consider the system of equations
    \begin{align*}
      x_1 - 1 &= 0, \\
      x_1x_2 - 1 &= 0.
    \end{align*}
    For what starting point or points,
    if any,
    will Newton's method for solving this system fail?
    Why?
  \end{pro}
  
  \begin{sol}
    The Jacobian matrix is
    \begin{displaymath}
      J(x_1, x_2) =
      \begin{bmatrix}
        1 & 0 \\
        x_2 & x_1
      \end{bmatrix}
    \end{displaymath}
    When $x_1=0$, 
    it is singular,
    and Newton's method will fail.
  \end{sol}

  \begin{pro}
    Given the three data points $(-1, 1), (0, 0), (1, 1)$,
    determine the interpolating polynomial of degree two:
    \begin{itemize}
    \item[(a)]
      Using the monomial basis
  
    \item[(b)]
      Using the Lagrange basis
  
    \item[(c)]
      Using the Newton basis
    \end{itemize}
    Show that the three representations give the same polynomial.
  \end{pro}
  \begin{sol}
    \begin{itemize}
    \item[(a)]
      Solving the following system of linear equations
      \begin{displaymath}
        \begin{bmatrix}
          1 & -1 & 1 \\
          1 & 0 & 0 \\
          1 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
          x_1 \\
          x_2 \\
          x_3
        \end{bmatrix}
        =
        \begin{bmatrix}
          1 \\
          0 \\
          1
        \end{bmatrix}
      \end{displaymath}
      get $x_1 = 0, x_2 = 0, x_3 = 1$,
      means the interpolating polynomial is
      \begin{displaymath}
        p_2(t) = t^2.
      \end{displaymath}
  
    \item[(b)]
      By Lagrange interpolation polynomial, we have
      \begin{align*}
        p_2(t) &= 1\cdot \frac{(t-0)(t-1)}{(-1-0)(-1-1)}
        + 0\cdot \frac{(t+1)(t-1)}{(0+1)(0-1)}
                 + 1\cdot \frac{(t+1)(t-0)}{(1+1)(1-0)} \\
               &= \frac{t(t-1)}{2} + \frac{t(t+1)}{2} 
        = t^2.
      \end{align*}
  
    \item[(c)]
       From the table of divided differences
  \begin{center}
  \begin{tabular}{c|cccc}
  $t$  & $y$ \\
  \hline 
  -1 & 1 \\
  0 & 0 & -1 \\
  1 & 1 & 1 & 1
  \end{tabular}
  \end{center}
  , and by Newton's formula we have polynomial
  \begin{displaymath}
    p_2(t) = 1 - (t+1) + (t+1)t = t^2.
  \end{displaymath}
  
  It's obvious that the above three representations give the same polynomial.
    \end{itemize}
  \end{sol}
    
  \begin{pro}
    Write a formal algorithm for evaluating a polynomial
    at a given argument using Horner's nested evaluation scheme
    \begin{itemize}
    \item[(a)]
      For a polynomial expressed in terms of the monomial basis
  
    \item[(b)]
      For a polynomial expressed in Newton form
    \end{itemize}
  \end{pro}
  \begin{sol}
    \begin{itemize}
  
      \item[(a)] \
    \begin{algorithm}[htb]
    \caption{Evaluate a polynomial expressed in monomial basis form}
    \begin{algorithmic}[1]
%       \renewcommand{\algorithmicrequire}{\textbf{Input : }}
%       \Require $\mathbf{a} =
%       \begin{bmatrix}
%         a_0 & a_1 & \cdots & a_n
%       \end{bmatrix}^T
%   $ and $t$
    %  \renewcommand{\algorithmicrequire}{\textbf{Precondition : }}
    %   \renewcommand{\algorithmicensure}{\textbf{Output : }}
    %   \Ensure $y = p(t, \mathbf{a})$
      \State
      $y\leftarrow 0$
      \For{$i=n$ to $1$}
      \State $y \leftarrow ty + a_i$
      \EndFor
    \end{algorithmic}
  \end{algorithm}
  
  \item[(b)] \
    \begin{algorithm}[htb]
      \caption{Evaluate polynomial expressed in Newton form}
    \begin{algorithmic}[1]
    %   \renewcommand{\algorithmicrequire}{\textbf{Input : }}
    %   \Require $\mathbf{a} =
    %   \begin{bmatrix}
    %     a_0 & a_1 & \cdots & a_n
    %   \end{bmatrix}^T
    %   $, $\mathbf{t} =
    %   \begin{bmatrix}
    %     t_0 & t_1 & \cdots & t_{n-1}
    %   \end{bmatrix}
    %   $ and $t$
    %  \renewcommand{\algorithmicrequire}{\textbf{Precondition : }}
    %   \renewcommand{\algorithmicensure}{\textbf{Output : }}
    %   \Ensure $y = p(t, \mathbf{a}, \mathbf{t})$
      \State
      $y\leftarrow 0$
      \For{$i=n$ to $1$}
      \State $y \leftarrow (t-t_i)y + a_i$
      \EndFor
    \end{algorithmic}
  \end{algorithm}  
  \end{itemize}
  \end{sol}

  \begin{pro}
    Use Lagrange interpolation to derive the formulas given in
    Section 5.5.5 for inverse quadratic interpolation.
  \end{pro}
  
  \begin{sol}
    The interpolation condition is
    \begin{displaymath}
      p_2(f_a) = a, \quad p_2(f_b) = b, \quad p_2(f_c) = c.
    \end{displaymath}
    Applying the Lagrange interpolation polynomial gets
    \begin{displaymath}
      p_2(y) = a \frac{(y-f_b)(y-f_c)}{(f_a-f_b)(f_a-f_c)}
      + b \frac{(y-f_a)(y-f_c)}{(f_b-f_a)(f_b-f_c)}
      + c \frac{(y-f_a)(y-f_b)}{(f_c-f_a)(f_c-f_b)},
    \end{displaymath}
    Replace with $y=0$ gives
    \begin{align*}
      p_2(0) &= a \frac{f_bf_c}{(f_a-f_b)(f_a-f_c)}
      + b \frac{f_af_c}{(f_b-f_a)(f_b-f_c)}
               + c \frac{f_af_b}{(f_c-f_a)(f_c-f_b)} \\
      &= b + \frac{v(w(u-w)(c-b) - (1-u)(b-a))}{(w-1)(u-1)(v-1)} \qquad \text{by definecolor of } p,q\\ 
      &= b + p/q,
    \end{align*}
    where
    \begin{displaymath}
      u = \frac{f_b}{f_c}, \quad v = \frac{f_b}{f_a}, \quad
      w = \frac{f_a}{f_c}.
    \end{displaymath}
  \end{sol}

  \begin{pro}
    Prove that the formula using divided differences given in Section
    7.3.3,
    \begin{displaymath}
      x_j = f[t_1, t_2, \ldots, t_j],
    \end{displaymath}
    indeed gives the coefficient of the $j$th basis function
    in the Newton polynomial interpolant.
  \end{pro}
  
  \begin{proof}
    We utilize a mathematical induction on $j$.
    \begin{itemize}
    \item
      For $j=1$, the interpolating polynomial is
      \begin{displaymath}
        p_1(t) = x_1 = f(t_1).
      \end{displaymath}
  
    \item
      Suppose the conclusion is true for all integers less than $n$,
      we show that it holds for $n+1$ as well.
      
      By our inductive hypothesis, we know that
      the polynomial interpolating
      \begin{displaymath}
        p_n(t_1) = f(t_1), \quad p_n(t_2) = f(t_2), \quad p_n(t_n) = f(t_n)
      \end{displaymath}
      is given by
      \begin{displaymath}
        p_n(t) = \sum_{i=1}^nx_i\prod_{j=1}^{i-1}(t-t_j)
        = \sum_{i=1}^nf[t_1, \ldots, t_i]\prod_{j=1}^{i-1}(t-t_j).
      \end{displaymath}
      Similarly,
      \begin{displaymath}
        q_n(t_2) = f(t_2), \quad q_n(t_3) = f(t_3), \quad q_n(t_{n+1}) = f(t_{n+1})
      \end{displaymath}
      is given by
      \begin{displaymath}
        q_n(t) = \sum_{i=1}^nx_i\prod_{j=2}^i(t-t_j)
        = \sum_{i=1}^nf[t_2, \ldots, t_{i+1}]\prod_{j=2}^i(t-t_j).
      \end{displaymath}
      Therefore from the uniqueness of the interpolating polynomial,
      we know that the polynomial for interpolating
      \begin{displaymath}
        p_{n+1}(t_1) = f(t_1), \quad p_{n+1}(t_2) = f(t_2), \quad
        p_{n+1}(t_n) = f(t_n), \quad p_{n+1}(t_{n+1}) = f(t_{n+1})
      \end{displaymath}
      is given by
      \begin{displaymath}
        p_{n+1}(t) = \frac{t-t_{n+1}}{t_1-t_{n+1}}p_n(t)
        + \frac{t-t_1}{t_{n+1}-t_1}q_n(t)
      \end{displaymath}
      Comparing the coefficient of the highest-order term of the above
      two polynomials gives
      \begin{displaymath}
        x_{n+1} = \frac{f[t_2, t_3, \ldots, t_{n+1}]-f[t_1, t_2, \ldots,
          t_n]}{t_{n+1} - t_1} = f[t_1, t_2, \ldots, t_{n+1}],
      \end{displaymath}
      where the second equality follows from the definition of divided differences.
      Therefore we have shown that the conclusion holds for $n+1$,
      which completes the inductive proof.
    \end{itemize}
  \end{proof}

  \begin{pro}
    Verify the properties of B-splines enumerated in Section 7.4.3.
  \end{pro}
  \begin{sol}
      \begin{itemize}
          \item [1] By mathematical induction 
          \begin{itemize}
              \item For $k = 0$, $B_i^0(t) = 0$ is obvious.
              \item Assuming for $k = 0,\ldots n$, $t<t_i$ or $t > t_{i+k+1}$ imply $B_i^k = 0$.
              By induction hypothesis, we have $B_i^n(t) = 0, B_{i + 1}^n(t) = 0$, so 
              \[B_i^{n+1}(t) = v_i^{n + 1}(t)B_i^n(t) +v_{i+1}^{n + 1}(t) = 0.\] 
              Therefore we have $B_i^k(t) = 0, t < t_i \text{ or } t> t_{i + k+ 1}$.
          \end{itemize}

          \item [2] Similarly assuming $B_i^k(t) > 0$ as in 1 in 
          different domain, we 
          have $B_i^k(t) > 0, t \in [t_i, t_{i + k +1}]$.

          \item [3] \begin{itemize}
              \item For $k = 0, \forall t$, $\sum_{i - -\infty}^\infty B_i^0(t) = 1$ is obvious.
              \item Assuming for $k = n, \forall t$, the assumption is right.
              \begin{align*}
                \sum_{i - -\infty}^\infty B_i^{n + 1}(t) &= 
                \sum_{i - -\infty}^\infty (v_i^{n + 1}(t)B_i^n(t) + v_{i + 1}^{n + 1}(t)B_{i+1}^n(t)) \\
                &=\sum_{i - -\infty}^\infty B_i^n(t) (v_i^{n+1}(t)
                + (1 - v_i^{n+1}) ) = \sum_{i - -\infty}^\infty B_i^n(t) = 1
              \end{align*}
          \end{itemize}

          \item [4]  We prove the following theorem:
          \begin{thm}
            For $k\ge 2$, we have,
            $\forall t\in\mathbb{R}$,
            \begin{equation}
              \label{eq:2}
              \frac{\dif}{\dif t}B_i^k(t) = \frac{kB_i^{k-1}(t)}{t_{i+k}-t_i} -
              \frac{kB_{i+1}^{k-1}(t)}{t_{i+k+1}-t_{i+1}}.
            \end{equation}
            For $k=1$, \eqref{eq:2} holds for all $t$ except at the three knots
            $t_i, t_{i+1}, t_{i+2}$,
            where the derivative of $B_i^1$ is not defined.
          \end{thm}
          \begin{proof}[Proof of Theorem]
            We first show that \eqref{eq:2} holds for all $t$ except at the knots $t_j$.
            \begin{displaymath}
              \forall t\in\mathbb{R}\backslash\{t_i, t_{i+1}, t_{i+2}\},\quad
              \frac{\dif}{\dif t}B_i^1(t) = \frac{1}{t_{i+1}-t_i}B_i^0(t)
              - \frac{1}{t_{i+2}-t_{i+1}}B_{i+1}^0(t).
            \end{displaymath}
            Hence the induction hypothesis holds.
            Now suppose \eqref{eq:2} holds $\forall t\in\mathbb{R}\backslash
            \{t_i, \ldots, t_{i+k+1}\}$.
            apply the induction hypothesis \eqref{eq:2},
            and we have
            \begin{equation}
              \label{eq:3}
              \frac{\dif}{\dif t}B_i^{k+1}(t) = \frac{B_i^k(t)}{t_{i+k+1}-t_i} -
      \frac{B_{i+1}^k(t)}{t_{i+k+2}-t_{i+1}} + kC(t)
      \end{equation}
      where
      \begin{align*}
        C(t) &= \frac{t-t_i}{t_{i+k+1}-t_i}\left[\frac{B_i^{k-1}(t)}{t_{i+k}-t_i}
        - \frac{B_{i+1}^{k-1}(t)}{t_{i+k+1}-t_{i+1}}\right] +
        \frac{t_{i+k+2}-t}{t_{i+k+2}-t_{i+1}}\left[\frac{B_{i+1}^{k-1}(t)}{t_{i+k+1}-t_{i+1}}
               - \frac{B_{i+2}^{k-1}(t)}{t_{i+k+2}-t_{i+2}}\right] \\
             &=
               \frac{1}{t_{i+k+1}-t_i}\left[\frac{(t-t_i)B_i^{k-1}(t)}{t_{i+k}-t_i}
                 +
                 \frac{(t_{i+k+1}-t)B_{i+1}^{k-1}(t)}{t_{i+k+1}-t_{i+1}}\right] \\
               &\quad- \frac{1}{t_{i+k+2}-t_{i+1}}\left[\frac{(t-t_{i+1})B_{i+1}^{k-1}(t)}{t_{i+k+1}-t_{i+1}}
               + \frac{(t_{i+k+2}-t)B_{i+2}^{k-1}(t)}{t_{i+k+2}-t_{i+2}}\right] 
       ]
        \\
        &= \frac{B_i^k(t)}{t_{i+k+1}-t_i} - \frac{B_{i+1}^k(t)}{t_{i+k+2}-t_{i+1}},
      \end{align*}
      Then \eqref{eq:3} can be written as
      \begin{displaymath}
        \frac{\dif}{\dif t}B_i^{k+1}(t) = \frac{(k+1)B_i^k(t)}{t_{i+k+1}-t_i} -
        \frac{(k+1)B_{i+1}^k(t)}{t_{i+k+2}-t_{i+1}},
      \end{displaymath}
      which completes the inductive proof of \eqref{eq:2} except at the knots.
      Since $B_i^1(t)$ is continuous,
      an easy induction shows that $B_i^k$ is
      continuous for all $k\ge 1$.
      Hence the right-hand side of \eqref{eq:2} is continuous for all $k\ge 2$.
      Therefore,
      if $k\ge 2$, $\frac{\dif}{\dif t}B_i^k(t)$ exists for all $t\in\mathbb{R}$.
      This completes the proof of the theorem.
    \end{proof}
    The proof follows from the above theorem and a simple induction on $k$.
    \item [5] By Theorem 2.1 and assume there is $c_i$ such that 
     \[\sum_{i = -\infty}^\infty c_i B_i^k(t) = 0.\]
     We have 
\[\frac{d}{dt}\sum_{i = -\infty}^\infty c_i B_i^k(t)  = \sum_{i = -\infty}^\infty (\frac{c_i - c_{i
- 1}}{t_{i+k} - t_{i}} B_i^{k -1}(t)) = 0.\]
Combining with mathematical induction and obvious situation $k = 0$, 
which means $B_i^{k - 1}$ is linearly independent.
So we have the coefficients $ \forall i,\frac{c_i - c_{i- 1}}{t_{i+k} - t_{i}} = 0 \Rightarrow c_i =
C$. Then 
\[\sum_{i = -\infty}^\infty C B_i^{k-1}(t) = 0\]
is contradiction with property 3.

        \item [6] Since there is $(k + 1)*2$ coefficient unknowns
        in two spline functions have $k-1$ times continuous differentiable.
        And in other side there is $k - 1$ continuous differentiable condition equations
        and $k + 2$ independent B-splines coefficients(By property 1, 5), At 
        last there is one value condition in $t_i$. 
        So $(k +1) * 2 = (k - 1) + (k + 2) + 1$ imply there always is a unique solution.
        Which also means $B_i^k, i \in N$ is a basis span the $k-1$ times 
        continuous differentiable spline function.
      \end{itemize}
  \end{sol}
\end{document}
